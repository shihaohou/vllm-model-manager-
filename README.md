# vLLM Model Manager

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-3.0+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

ä¸€ä¸ªä¼˜é›…çš„ Web ç®¡ç†ç³»ç»Ÿï¼Œç”¨äºç®¡ç†å’Œç›‘æ§ vLLM æ¨¡å‹æœåŠ¡

[åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) â€¢ [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜) â€¢ [ç•Œé¢é¢„è§ˆ](#ç•Œé¢é¢„è§ˆ) â€¢ [APIæ–‡æ¡£](#apiæ–‡æ¡£)

</div>

---

## åŠŸèƒ½ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½

- ğŸš€ **ä¸€é”®ç®¡ç†** - é€šè¿‡ Web ç•Œé¢ä¸€é”®å¯åŠ¨/åœæ­¢æ‰€æœ‰æ¨¡å‹æœåŠ¡
- ğŸ“Š **å®æ—¶ç›‘æ§** - å®æ—¶æ˜¾ç¤º GPU åˆ©ç”¨ç‡ã€æ˜¾å­˜ä½¿ç”¨ã€æ¸©åº¦ã€åŠŸè€—ç­‰
- ğŸ’» **ç³»ç»Ÿèµ„æº** - ç›‘æ§ CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æƒ…å†µ
- ğŸ“ **æ—¥å¿—æŸ¥çœ‹** - åœ¨ Web ç•Œé¢ç›´æ¥æŸ¥çœ‹æœåŠ¡æ—¥å¿—
- ğŸ¨ **ç°ä»£åŒ– UI** - ç¾è§‚çš„æ¸å˜è‰²ç•Œé¢ï¼Œå“åº”å¼è®¾è®¡
- ğŸ”„ **è‡ªåŠ¨åˆ·æ–°** - æ¯ 5 ç§’è‡ªåŠ¨æ›´æ–°æ‰€æœ‰çŠ¶æ€ä¿¡æ¯
- âš™ï¸ **çµæ´»é…ç½®** - é€šè¿‡ JSON é…ç½®æ–‡ä»¶è½»æ¾ç®¡ç†å¤šä¸ªæœåŠ¡

### æ”¯æŒçš„åŠŸèƒ½

- âœ… å¤šæœåŠ¡ç®¡ç†
- âœ… GPU å®æ—¶ç›‘æ§
- âœ… è¿›ç¨‹çŠ¶æ€è¿½è¸ª
- âœ… èµ„æºä½¿ç”¨ç»Ÿè®¡
- âœ… æ—¥å¿—å®æ—¶æŸ¥çœ‹
- âœ… é…ç½®æ–‡ä»¶é©±åŠ¨

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- NVIDIA GPU with CUDA
- vLLM å·²å®‰è£…
- nvidia-smi å¯ç”¨

### å®‰è£…ä¾èµ–

```bash
pip install flask flask-cors psutil
```

### é…ç½®æœåŠ¡

1. å¤åˆ¶é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š

```bash
cp config/services.json.example config/services.json
```

2. ç¼–è¾‘ `config/services.json`ï¼Œé…ç½®ä½ çš„æ¨¡å‹æœåŠ¡ï¼š

```json
{
    "service_key": {
        "name": "æ¨¡å‹åç§°",
        "port": 8000,
        "start_script": "/path/to/start_script.sh",
        "stop_script": "/path/to/stop_script.sh",
        "log_file": "/path/to/log_file.log",
        "process_pattern": "vllm serve.*YourModel",
        "gpus": [0, 1]
    }
}
```

### å¯åŠ¨ç®¡ç†ç³»ç»Ÿ

```bash
python3 app.py
```

é»˜è®¤è®¿é—®åœ°å€: **http://0.0.0.0:9000**

### è‡ªå®šä¹‰ç«¯å£å’Œä¸»æœº

```bash
# è‡ªå®šä¹‰ç«¯å£
PORT=8080 python3 app.py

# è‡ªå®šä¹‰ä¸»æœºå’Œç«¯å£
HOST=127.0.0.1 PORT=8080 python3 app.py
```

## é…ç½®è¯´æ˜

### æœåŠ¡é…ç½®é¡¹

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `name` | string | æœåŠ¡æ˜¾ç¤ºåç§° |
| `port` | int | æœåŠ¡ç›‘å¬ç«¯å£ |
| `start_script` | string | å¯åŠ¨è„šæœ¬çš„ç»å¯¹è·¯å¾„ |
| `stop_script` | string | åœæ­¢è„šæœ¬çš„ç»å¯¹è·¯å¾„ |
| `log_file` | string | æ—¥å¿—æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ |
| `process_pattern` | string | ç”¨äºè¯†åˆ«è¿›ç¨‹çš„æ­£åˆ™è¡¨è¾¾å¼ |
| `gpus` | array | æœåŠ¡ä½¿ç”¨çš„ GPU ç¼–å·åˆ—è¡¨ |

### é…ç½®ç¤ºä¾‹

```json
{
    "llama3": {
        "name": "LLaMA-3-70B",
        "port": 8000,
        "start_script": "/opt/models/scripts/start_llama3.sh",
        "stop_script": "/opt/models/scripts/stop_llama3.sh",
        "log_file": "/var/log/vllm/llama3.log",
        "process_pattern": "vllm serve.*llama-3-70b",
        "gpus": [0, 1, 2, 3]
    },
    "mistral": {
        "name": "Mistral-7B",
        "port": 8001,
        "start_script": "/opt/models/scripts/start_mistral.sh",
        "stop_script": "/opt/models/scripts/stop_mistral.sh",
        "log_file": "/var/log/vllm/mistral.log",
        "process_pattern": "vllm serve.*mistral",
        "gpus": [4]
    }
}
```

## ç•Œé¢é¢„è§ˆ

### ä¸»ç•Œé¢

- **ç³»ç»Ÿèµ„æºç›‘æ§é¢æ¿** - æ˜¾ç¤º CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æƒ…å†µ
- **GPU çŠ¶æ€å¡ç‰‡** - æ¯ä¸ª GPU çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåˆ©ç”¨ç‡ã€æ˜¾å­˜ã€æ¸©åº¦ã€åŠŸè€—ï¼‰
- **æœåŠ¡ç®¡ç†å¡ç‰‡** - æ¯ä¸ªæœåŠ¡çš„çŠ¶æ€å’Œæ§åˆ¶æŒ‰é’®

### æœåŠ¡å¡ç‰‡ä¿¡æ¯

è¿è¡Œä¸­çš„æœåŠ¡æ˜¾ç¤ºï¼š
- è¿›ç¨‹ PID
- CPU ä½¿ç”¨ç‡
- å†…å­˜å ç”¨
- GPU æ˜¾å­˜å ç”¨
- å¯åŠ¨æ—¶é—´

### æ“ä½œæŒ‰é’®

- **å¯åŠ¨** - å¯åŠ¨æ¨¡å‹æœåŠ¡
- **åœæ­¢** - åœæ­¢æ¨¡å‹æœåŠ¡
- **æ—¥å¿—** - æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼ˆæœ€è¿‘ 200 è¡Œï¼‰

## API æ–‡æ¡£

### è·å–æ‰€æœ‰æœåŠ¡çŠ¶æ€

```http
GET /api/services
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
    "service_key": {
        "name": "æ¨¡å‹åç§°",
        "port": 8000,
        "gpus": [0, 1],
        "status": "running",
        "pid": 12345,
        "cpu_percent": 5.2,
        "memory_mb": 2048.5,
        "gpu_memory_mb": 10240,
        "uptime": "2024-01-20 10:30:00"
    }
}
```

### è·å– GPU ä¿¡æ¯

```http
GET /api/gpu
```

å“åº”ç¤ºä¾‹ï¼š
```json
[
    {
        "index": 0,
        "name": "NVIDIA A100-SXM4-80GB",
        "utilization": 85.5,
        "memory_used": 65536,
        "memory_total": 81920,
        "temperature": 68.0,
        "power_draw": 320.5,
        "power_limit": 400.0
    }
]
```

### è·å–ç³»ç»Ÿä¿¡æ¯

```http
GET /api/system
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
    "cpu_percent": 45.2,
    "memory_percent": 62.8,
    "memory_used_gb": 50.5,
    "memory_total_gb": 128.0,
    "disk_percent": 35.6,
    "disk_used_gb": 450.2,
    "disk_total_gb": 1000.0
}
```

### å¯åŠ¨æœåŠ¡

```http
POST /api/service/{service_key}/start
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
    "success": true,
    "message": "æœåŠ¡å¯åŠ¨æˆåŠŸ",
    "output": "..."
}
```

### åœæ­¢æœåŠ¡

```http
POST /api/service/{service_key}/stop
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
    "success": true,
    "message": "æœåŠ¡åœæ­¢æˆåŠŸ",
    "output": "..."
}
```

### è·å–æœåŠ¡æ—¥å¿—

```http
GET /api/service/{service_key}/logs?lines=100
```

å‚æ•°ï¼š
- `lines` (å¯é€‰): è¿”å›çš„æ—¥å¿—è¡Œæ•°ï¼Œé»˜è®¤ 100

å“åº”ç¤ºä¾‹ï¼š
```json
{
    "success": true,
    "logs": "æ—¥å¿—å†…å®¹..."
}
```

## é¡¹ç›®ç»“æ„

```
vllm-model-manager/
â”œâ”€â”€ app.py                      # Flask åº”ç”¨ä¸»ç¨‹åº
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html             # Web ç•Œé¢æ¨¡æ¿
â”œâ”€â”€ static/                    # é™æ€èµ„æºç›®å½•ï¼ˆé¢„ç•™ï¼‰
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ services.json          # æœåŠ¡é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ services.json.example  # é…ç½®æ–‡ä»¶ç¤ºä¾‹
â”œâ”€â”€ requirements.txt           # Python ä¾èµ–
â”œâ”€â”€ .gitignore                # Git å¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ LICENSE                   # å¼€æºåè®®
â””â”€â”€ README.md                 # é¡¹ç›®æ–‡æ¡£
```

## éƒ¨ç½²å»ºè®®

### ä½¿ç”¨ systemd ç®¡ç†

åˆ›å»º `/etc/systemd/system/vllm-manager.service`ï¼š

```ini
[Unit]
Description=vLLM Model Manager
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/opt/vllm-model-manager
Environment="PATH=/usr/local/bin:/usr/bin:/bin"
ExecStart=/usr/bin/python3 /opt/vllm-model-manager/app.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

å¯åŠ¨æœåŠ¡ï¼š
```bash
sudo systemctl daemon-reload
sudo systemctl enable vllm-manager
sudo systemctl start vllm-manager
```

### ä½¿ç”¨ Nginx åå‘ä»£ç†

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:9000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

### æ·»åŠ åŸºç¡€è®¤è¯

å®‰è£… `flask-httpauth`ï¼š
```bash
pip install flask-httpauth
```

åœ¨ `app.py` ä¸­æ·»åŠ ï¼š
```python
from flask_httpauth import HTTPBasicAuth

auth = HTTPBasicAuth()

users = {
    "admin": "password"
}

@auth.verify_password
def verify_password(username, password):
    if username in users and users[username] == password:
        return username

# åœ¨è·¯ç”±ä¸Šæ·»åŠ  @auth.login_required è£…é¥°å™¨
```

## å®‰å…¨å»ºè®®

1. **é˜²ç«å¢™é…ç½®** - é™åˆ¶ç®¡ç†ç«¯å£çš„è®¿é—®
2. **ä½¿ç”¨ HTTPS** - é…ç½® SSL è¯ä¹¦
3. **æ·»åŠ è®¤è¯** - ä½¿ç”¨ HTTP åŸºç¡€è®¤è¯æˆ– OAuth
4. **å®šæœŸæ›´æ–°** - åŠæ—¶æ›´æ–°ä¾èµ–åŒ…
5. **æ—¥å¿—å®¡è®¡** - è®°å½•æ‰€æœ‰æ“ä½œæ—¥å¿—

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆ GPU ä¿¡æ¯æ— æ³•æ˜¾ç¤ºï¼Ÿ

A: ç¡®ä¿ `nvidia-smi` å‘½ä»¤å¯ç”¨ï¼Œå¹¶ä¸”å½“å‰ç”¨æˆ·æœ‰æƒé™æ‰§è¡Œè¯¥å‘½ä»¤ã€‚

### Q: æœåŠ¡å¯åŠ¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
- å¯åŠ¨è„šæœ¬è·¯å¾„æ˜¯å¦æ­£ç¡®
- å¯åŠ¨è„šæœ¬æ˜¯å¦æœ‰æ‰§è¡Œæƒé™
- GPU æ˜¯å¦è¢«å…¶ä»–è¿›ç¨‹å ç”¨
- ç«¯å£æ˜¯å¦è¢«å ç”¨
- æŸ¥çœ‹æœåŠ¡æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯

### Q: å¦‚ä½•æ·»åŠ æ–°çš„æœåŠ¡ï¼Ÿ

A: åœ¨ `config/services.json` ä¸­æ·»åŠ æ–°çš„æœåŠ¡é…ç½®ï¼Œç„¶åé‡å¯ç®¡ç†ç³»ç»Ÿã€‚

### Q: å¯ä»¥ç®¡ç†é vLLM æœåŠ¡å—ï¼Ÿ

A: å¯ä»¥ï¼åªéœ€è¦æä¾›å¯¹åº”çš„å¯åŠ¨/åœæ­¢è„šæœ¬å’Œè¿›ç¨‹è¯†åˆ«æ¨¡å¼å³å¯ã€‚

## å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒ Docker éƒ¨ç½²
- [ ] æ·»åŠ ç”¨æˆ·è®¤è¯ç³»ç»Ÿ
- [ ] æ€§èƒ½å›¾è¡¨å’Œå†å²æ•°æ®
- [ ] é‚®ä»¶/é’‰é’‰å‘Šè­¦é€šçŸ¥
- [ ] API å¯†é’¥ç®¡ç†
- [ ] å¤šèŠ‚ç‚¹æ”¯æŒ
- [ ] æœåŠ¡å¥åº·æ£€æŸ¥
- [ ] è‡ªåŠ¨æ•…éšœæ¢å¤

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## è‡´è°¢

- [Flask](https://flask.palletsprojects.com/) - Web æ¡†æ¶
- [vLLM](https://github.com/vllm-project/vllm) - å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“
- [psutil](https://github.com/giampaolo/psutil) - ç³»ç»Ÿå’Œè¿›ç¨‹å·¥å…·

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ–é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- GitHub Issues: [é¡¹ç›® Issues é¡µé¢](https://github.com/yourusername/vllm-model-manager/issues)

---

<div align="center">

**[â¬† å›åˆ°é¡¶éƒ¨](#vllm-model-manager)**

Made with â¤ï¸ for the AI community

</div>
